{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "These blue boxes contain instructions for you to follow, or stuff for you to do\n",
    "<h2>How to access this Jupyter notebook</h2>\n",
    "\n",
    "* <b>Step 1</b>: Open a web browser,  and go to [this page](https://mnf144.csc.warwick.ac.uk:8987/module/CH274), \n",
    "* <b>Step 2</b>: Enter your SCRTP username and password and press the \"Start Server\" button.<br>\n",
    "* <b>Step 3</b>: Wait (it could take a few minutes) until the blue blox says \"Jupyter notebook server running!\". At that point, click on the weblink below said message.<br>\n",
    "* <b>Step 4</b>: Select the Jupyter Notebook you want to work on. Remember to make a copy of the orginal notebook (which is read only). To do so, in the toolbar on top of the notebook, select File and then Make a Copy <br>\n",
    "* <b>Step 5</b>: You're all set! <br>\n",
    "* <b>Step 6</b>: <font color=\"red\">When you are done, remember to click the \"Stop Server\" button in the launcher web browser tab.</font> Please do, it's really quite important. <br>\n",
    "<b> Remember: </b> You can access your copy of the Notebook at any time from any device off and on campus by going through the same steps on e.g. your laptop - all the changes you have made will be saved and synced! <br>\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH274 Computational Workshop 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to data fitting\n",
    "It is often the case that in order to extract useful information from experimental data we need to fit it first. You have already met examples of this in the undergraduate labs (Iodination of acetone experiment) and the CH271 key skills workshop. \n",
    "\n",
    "## Real example of curve fitting\n",
    "Examples of how curve fitting is used in a real phyical chemistry experiment are shown below. The video shows results obtained from a time-resolved infrared absorption experiment where a sample has been excited to its S$_{1}$ electronic state using a UV laser, and then probed after a series of time-delays (between 500 fs and 1000 ns) using an IR laser. The resulting spectra are very complicated, with vibrational peaks belonging to several different molecular isomers (in more than one electronic state) contributing to the signal. In addition to this, the size of the peaks change, and new features appear, as the pump/probe time delay increases:\n",
    "\n",
    "\n",
    "<video  width=\"800\" height=\"600\" controls src=\"./STUFF/FitMov.mov\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the movie above, the experimental data points are the open circles. We can see that each of the time-resolved spectra have overlapping contributions from up to four different species. We can model these contributions using curve fitting (each component is represented as a sum of Gaussian functions - plotted as dashed lines), with the total fit to the experimental data shown by the solid blue line.\n",
    "\n",
    "The intensities of the four components as a function of pump/probe time delay provide us with information about the kinetics of the system, allowing us to determine the primary quantum yields for the various decay pathways of the S$_{1}$ electronic state. In order to extract this detailed information however, we have to perform another series of curve fits to access the rate coefficients. Such data analysis often takes far longer to perform than the experiment itself!\n",
    "\n",
    "## This session\n",
    "In this session we will learn how the ```curve_fit``` package of the [SciPy](https://www.scipy.org) module can be used to fit experimental data. You will need to apply these skills over the Easter break in the final CH274 computational assignment, so it is vital that you read the material <b>carefully</b>, and attempt all five tasks (either in the session or your own time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the modules we will need for the workshop, and give them aliases\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Since we will only be using the 'curve_fit' package from SciPy, we will load this module alone rather \n",
    "#than the entire SciPy library\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Straight line fits\n",
    "The simplest example of curve fitting, and the one you are probably most familiar with, is where we fit data to a straight line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR FITTING EXAMPLE - CREATING OUR DATA\n",
    "\n",
    "#create x- and y-values lying roughly along a straight line\n",
    "xdata_linear=np.array([0,1,2,3,4,5])\n",
    "ydata_linear=np.array([0.5,2.23,3.7,5.8,6.9,8.3])\n",
    "\n",
    "#plot the data as a scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xdata_linear,ydata_linear,s=60, c='#D9514E',label='Data points') #s=60 changes the size of the points\n",
    "\n",
    "#Add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('x',fontsize=18)\n",
    "plt.ylabel('y',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Defining our fit function\n",
    "To fit our data, we need to define a model function - the mathematical equation we are fitting our data to. Since we are fitting a straight line, we will make use of the usual equation for such a line: $$y(x)=mx+c$$\n",
    "Where $x$ is the independent variable and $m$ (the gradient of the line) and $c$ (the y-intercept) are our fit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR FITTING EXAMPLE - DEFINING OUR FIT FUNCTION\n",
    "\n",
    "#define our model function (give it a sensible name), and declare the independent variable followed by the \n",
    "#fit parameters (x,m,c). \n",
    "\n",
    "def linFit(x,m,c): #note the semi-colon at the end of this line\n",
    "    return m*x+c   #equation for a straight line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Fitting the data\n",
    "Now that we have defined our model function, we can use it to fit our data ($i.e.$ find the best values for $m$ and $c$). To do this, we use the \"curve_fit\" command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR FITTING EXAMPLE - FITTING OUR DATA\n",
    "\n",
    "#Use curve_fit command (part of scipy) to fit our data.\n",
    "#Curve fit called with three arguments: the fit function (defined above), and the x and y-values \n",
    "#of the data we are fitting\n",
    "popt,pcov=curve_fit(linFit,xdata_linear,ydata_linear) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``curve_fit`` command returns two arrays as output, which we have chosen to call ```popt``` and ```pcov```, respectively. The 'popt' array contains the best-fit values for our fit-parameters (m and c). The order in which they appear in the array is the same as the order in which they were defined in the original fit function ('m' then 'c' in this case). We can view the best-fit values using the ```print(popt)``` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the gradient of our line is 1.57, with a y-intercept of 0.64. You should always check the quality of the fit by plotting the line of best fit along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR FITTING EXAMPLE - CHECKING THE QUALITY OF THE FIT\n",
    "\n",
    "#Generate the line of best fit - y=mx+c:\n",
    "yfit_linear=(popt[0]*xdata_linear)+popt[1] #popt[0]=m, popt[1]=c (remember that python counts from zero)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "#plot the data and line of best fit\n",
    "plt.scatter(xdata_linear,ydata_linear,s=60, c='#D9514E',label='Data points') #Original data\n",
    "plt.plot(xdata_linear, yfit_linear,lw=2,c='#2A2B2D', label='Fit') #Fit to data\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('x',fontsize=18)\n",
    "plt.ylabel('y',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "#Add line equation to the plot\n",
    "plt.text(3,3,\"y={:4.2f}x+{:4.2f}\".format(popt[0],popt[1]),fontsize=16) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. What about 'pcov'?\n",
    "The 'pcov' array generated above is the covariance matrix for the fit. In general, it will be a $n\\times n$ array, where $n$ is the number of fit-parameters in the model function (in our example it is a $2\\times 2$ array): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array contains all the information about the quality and reliability of our fit. The one standard-deviation uncertainties in our fit-parameters are the square-roots of the diagonal (top left to bottom right) elements. Again the order in which they appear are the order in which the fit parameters were defined in the original fit-function.\n",
    "\n",
    "We can extract these directly from ``pcov`` using the ```np.sqrt(np.diag(ArrayName))``` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR FITTING EXAMPLE - EXTRACTING FIT PARAMETER UNCERTAINITES\n",
    "\n",
    "Error=np.sqrt(np.diag(pcov)) #np.diag extracts the diagonal elements of an array, np.sqrt takes the square-root\n",
    "\n",
    "print('Gradient={:4.2f}, 1 S.D. uncertainty={:4.2f}'.format(popt[0],Error[0]))\n",
    "print('Intercept={:4.2f}, 1 S.D. uncertainty={:4.2f}'.format(popt[1],Error[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its always a good idea to look at the off-diagonal elements of the covariance matrix too. These numbers tell you the degree of correlation between your fit parameters. Ideally you want these to be close to zero (this tells you that your fit parameters are independent of each other). Values approaching $\\pm1$ indicate that the reliabaility of the fit is poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### <font>Task 1:</font>\n",
    "Fit the following data to a straight line (use the ```linFit()``` function defined above). Determine the best-fit values for the gradient of the line, and their associated one standard deviation errors. Plot the results of your fit, and report the best-fit values for $m$ and $c$ along with their one standard-deviation uncertanties.\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Task 1 data:\n",
    "x_task1=np.array([0,1,2,3,4,5])\n",
    "y_task1=np.array([0.2,2.7,4.9,7.3,8.7,11.4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Non-linear Curve FItting\n",
    "Oftentimes our data is not going to lie on a straight line, and so we need to fit it to a more complicated, non-linear, function. We achieve this in largely the same manner as we used for straight line fits.\n",
    "\n",
    "Lets demonstrate this by fitting an exponential decay of the form $y(x)=e^{-kx}+c$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONLINEAR FITTING EXAMPLE - CREATING OUR DATA\n",
    "\n",
    "#x- and y-values for exponentially decaying curve:\n",
    "xdata_nonlin=np.array([0.        , 0.20833333, 0.41666667, 0.625     , 0.83333333,\n",
    "       1.04166667, 1.25      , 1.45833333, 1.66666667, 1.875     ,\n",
    "       2.08333333, 2.29166667, 2.5       , 2.70833333, 2.91666667,\n",
    "       3.125     , 3.33333333, 3.54166667, 3.75      , 3.95833333,\n",
    "       4.16666667, 4.375     , 4.58333333, 4.79166667, 5.        ])\n",
    "ydata_nonlin=np.array([2.29571132, 2.07101355, 1.86747534, 1.7982708 , 1.6465569 ,\n",
    "       1.55285214, 1.51899243, 1.47650864, 1.38996595, 1.35309758,\n",
    "       1.3564634 , 1.34031022, 1.3479142 , 1.30895075, 1.32096505,\n",
    "       1.33756549, 1.27642251, 1.32362688, 1.24847808, 1.30583286,\n",
    "       1.32490246, 1.28569048, 1.32053396, 1.29062279, 1.30465078])\n",
    "\n",
    "#Plot the data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xdata_nonlin,ydata_nonlin,s=60, c='#D9514E', label='Data points')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('x',fontsize=18)\n",
    "plt.ylabel('y',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Defining the fit function\n",
    "Our first step is to  write a python function defining the equation we are fitting the data to:\n",
    "$$y=e^{-kx}+c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONLINEAR FITTING EXAMPLE - DEFINING OUR FIT FUNCTION\n",
    "\n",
    "#define our model function (expFit), and declare the independent variable, x, followed \n",
    "#by the two fit parameters, k & c\n",
    "\n",
    "def expFit(x,k,c): \n",
    "    return np.exp(-k*x)+c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Initial fit-parameter guesses\n",
    "The key difference between linear and non-linear fits is that we need to provide ```curve_fit``` with initial guesses for the fit parameters $k$ and $c$. We do this by creating an array and filling it with our initial guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONLINEAR FITTING EXAMPLE - INITIAL GUESSES FOR FIT PARAMETERS\n",
    "\n",
    "#our initial guesses for the values of k and c - if these are too far out the fit will not work\n",
    "guessParams=[9.0,4.8] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to plot our trial function against the original data to see if our initial guesses are reasonable. If they are too far off, the fit will not work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our trial function using our initial guesses for k and c which are stored in the array guessParams\n",
    "ytrial_nonlin=np.exp(-guessParams[0]*xdata_nonlin)+guessParams[1]\n",
    "\n",
    "#plot the trial function against the original data to see if our guesses are reasonable - the trial\n",
    "#function will roughly follow the data points if they are\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xdata_nonlin,ydata_nonlin,s=60, c='#D9514E',alpha=0.6, label='Data points')\n",
    "plt.plot(xdata_nonlin,ytrial_nonlin,lw=3,c='#2A2B2D', label='Initial guess')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('x',fontsize=18)\n",
    "plt.ylabel('y',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### <font>Task 2:</font>\n",
    "The initial guesses provided above are clearly terrible (the 'initial guess' line does not go anywhere near the data points). As things stand the curve-fitting procedure will not work! Change the values in the ``guessParams`` array defined above until you get a better match between the data and the initial guess (note: the guesses don't have to be perfect, this shouldn't take you more than a couple of minutes at most)\n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Curve fitting\n",
    "Once our guesses look good, then we can proceed with the curve fitting. The actual data-fitting process is nearly identical to that used in the example of a straight line fit. The only difference is that we need to include a new argument in the ```curve_fit``` command - the name of the array containing our guessed fit-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONLINEAR FITTING EXAMPLE - FITTING THE DATA\n",
    "\n",
    "#use the 'curve_fit' module to fit our data (xdata_nonlin,ydata_nonlin) using our user defined fit function (expFit) \n",
    "#with the initial guesses for the fit parameters (guessParams).\n",
    "#popt contains the best-fit parameters, pcov is used for error analysis\n",
    "popt, pcov = curve_fit(expFit, xdata_nonlin, ydata_nonlin, guessParams) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the resultant line of best fit shows if we have been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate line of best fit\n",
    "yfit_nonlin=np.exp(-popt[0]*xdata_nonlin)+popt[1]\n",
    "\n",
    "#Plot the data and line of best fit\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xdata_nonlin,ydata_nonlin,s=60, c='#D9514E',alpha=0.6, label='Data points')\n",
    "plt.plot(xdata_nonlin,yfit_nonlin,lw=3,c='#2A2B2D', label='Fit')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('x',fontsize=18)\n",
    "plt.ylabel('y',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Extract one standard deviation uncertainties for k and c from covariance matrix\n",
    "err=np.sqrt(np.diag(pcov))\n",
    "\n",
    "#Print the fit parameters along with their one standard deviation uncertainties\n",
    "print('k = {:4.2f}, 1 S.D. uncertainty={:4.2f}'.format(popt[0],err[0]))\n",
    "print('c = {:4.3f}, 1 S.D. uncertainty={:4.3f}'.format(popt[1],err[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### <font>Task 3:</font>\n",
    "The data in the file ```overlappingPeaks.csv``` describes a common situation in vibronic spectroscopy - that of overlapping spectral features. Read in the data from the ``STUFF`` folder (in the same directory as this python notebook), and plot the intensity (contained in the second column) against the wavenumber (first column).\n",
    "\n",
    "If you cannot remember how to load a .csv file, reacquaint yourself with section 2 of KS_4. \n",
    "    \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Task 3 answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    " \n",
    "### <font>Task 4:</font> \n",
    "We can decompose these two overlapping peaks using model functions (<i>c.f.</i> dashed lines in the video at the beginning of this notebook) to get the individual peak centres, peak widths, and peak intensities. In solution, vibrational line shapes are Gaussian functions:\n",
    "\n",
    "$$y(x)=A\\exp\\left(-\\left\\{\\frac{x-x_c}{w}\\right\\}^{2}\\right)$$\n",
    "\n",
    "where $A$ is the height of the Gaussian, $x_c$ is the $x$-value of the Gaussian's centre, and $w$ is its width.\n",
    "\n",
    "Since our simulated data consists of two peaks, our fit function will be the sum of two Gaussians. To simplify things, we can make the assumption that the width of the two Gaussian functions are identical. With this assumption, our overall fit function becomes:\n",
    "\n",
    "\n",
    "$$y(x)=A_1\\exp\\left(-\\left\\{\\frac{x-x_{c1}}{w}\\right\\}^{2}\\right)+A_{2}\\exp\\left(-\\left\\{\\frac{x-x_{c2}}{w}\\right\\}^{2}\\right)$$\n",
    "\n",
    "Use this function to fit the spectral data. The indpendent variable ($x$) is the wavenumber. Initial guesses for the five fit-parameters can be read directly from the plot produced in task 3 (<i>i.e.</i> $A_1$ and $A_2$ are roughly 2, the peak centres ($x_{c1}$ and $x_{c2}$) lie between 1730 and 1740 cm$^{-1}$, and the width of both Gaussians ($w$) is roughly 2 cm$^{-1}$). \n",
    "\n",
    "Plot the results of your fit, and report the best-fit values for the peak heights, widths, and peak centres along with their one standard-deviation uncertaities.\n",
    " \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4 answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simultaneous fitting of multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 introduced you to the concept of decomposing a vibrational spectrum into a series of model functions (in this case a different Gaussian for each vibrational peak). In a real time-resolved experiment, you would want to repeat this fit for every pump-probe time-delay, extract the peak areas (proportional to the concentration of the molecular species), and plot these areas as a function of pump-probe time delay. Since the primary photochemical pathways available to a molecule all obey first-order kinetics, these plots will display exponential decay/growth curves.\n",
    "\n",
    "## 3.1. A simple example: A &rarr; B first-order kinetics\n",
    "For example, consider the case where our initially created species, $A$, has a single decay pathway open to it (this could be fluorescence back down to the ground electronic state, dissociation into fragments, internal conversion, <i>etc.</i>) with a rate coefficient $k$:\n",
    "\n",
    "$$A\\xrightarrow{k}B$$\n",
    "\n",
    "The time-dependent concentrations of $A$ and $B$ are given by the following expressions (here we assume that only species $A$ is preset initially):\n",
    "\n",
    "$$\\begin{align*}\n",
    "    [A]_{t}&=[A]_{0}e^{-kt}\\\\\n",
    "    [B]_{t}&=[A]_{0}\\left(1-e^{-kt}\\right)\n",
    "   \\end{align*}$$\n",
    "  \n",
    "where $t$ is the pump/probe time delay, $[X]_{t}$ are the time-dependent concentrations of $A$ / $B$, and $[A]_{0}$ is the concentration of $A$ at $t=0$. \n",
    "\n",
    "Some simulated data is shown below (to simplify things $[A]_{0}=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the simulated data - AtoB.csv has three columns, each with column headers\n",
    "data=np.loadtxt('./STUFF/AtoB.csv',delimiter=',',skiprows=1)\n",
    "\n",
    "#Split the array to extract the data for plotting/fitting \n",
    "#time is column 1, concntration of A is column 2, and concentration of B is column 3\n",
    "time=data[:,0]\n",
    "ConcA=data[:,1]\n",
    "ConcB=data[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the simulated data\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(time,ConcA,s=60,c='#D9514E',label='[A]')\n",
    "plt.scatter(time,ConcB,s=60,c='#2A2B2D',label='[B]')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('Time / s$',fontsize=18) \n",
    "plt.ylabel('Concentration / a.u.',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit both of these curves separately using the methodology outlined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define two fit functions, one for the decay of A, and one describing the growth of B\n",
    "def expDecay(t,k):\n",
    "    return np.exp(-k*t)\n",
    "\n",
    "def expGrowth(t,k):\n",
    "    return 1-np.exp(-k*t)\n",
    "\n",
    "#initial guess for value of k \n",
    "guessParams=[0.5]\n",
    "\n",
    "#Fit the two data sets independently\n",
    "popt_decay,pcov_decay=curve_fit(expDecay,time,ConcA,guessParams) #[A] concentration\n",
    "popt_growth,pcov_growth=curve_fit(expGrowth,time,ConcB,guessParams)#[B] concentration\n",
    "\n",
    "#Generate best-fit curves\n",
    "fitA=np.exp(-popt_decay[0]*time)\n",
    "fitB=1-np.exp(-popt_growth[0]*time)\n",
    "\n",
    "#Plot the data and lines of best fit\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(time,ConcA,s=60,c='#D9514E',alpha=0.5,label='[A]')\n",
    "plt.plot(time,fitA,lw=3,c='#D9514E')\n",
    "plt.scatter(time,ConcB,s=60,c='#2A2B2D',alpha=0.5,label='[B]')\n",
    "plt.plot(time,fitB,lw=3,c='#2A2B2D')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('Time / s',fontsize=18) \n",
    "plt.ylabel('Concentration / a.u.',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print the best-fit parameters\n",
    "err_decay=np.sqrt(np.diag(pcov_decay))\n",
    "err_growth=np.sqrt(np.diag(pcov_growth))\n",
    "print('k (decay)={:4.3f}, 1 S.D uncertainty={:4.3f}'.format(popt_decay[0],err_decay[0]))\n",
    "print('k (growth)={:4.3f}, 1 S.D uncertainty={:4.3f}'.format(popt_growth[0],err_growth[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above fits have returned two different values for the same rate coefficient $k$! This is a common occurance because no experiment is perfect. The noise in the two curves above caused the fits to converge to two (slightly) different solutions. Which value should we use?\n",
    "\n",
    "We can avoid this problem by fitting both data sets simultaneously, thereby obtaining a single, global, best fit value for $k$. This approach bulids upon that used above, but we need to add in a couple of new steps.\n",
    "\n",
    "### (i) Define fit functions for each curve:\n",
    "We begin by defining our individual fit functions (expDecay and expGrowth) in the usual manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define two fit functions, one for the decay of A, and one describing the growth of B\n",
    "def expDecay(t,k):\n",
    "    return np.exp(-k*t)\n",
    "\n",
    "def expGrowth(t,k):\n",
    "    return 1-np.exp(-k*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Create two new arrays, one containing all the x-data, and one containing all the y-data:\n",
    "We next have to combine all of our concentration data into a single numpy array. This is achieved using the ```np.concatenate((dataset1,dataset2))``` command (note the use of <i>two</i> pairs of parentheses in this command) which adds 'dataset2' to the end of 'dataset1'. This step needs to be repeated for the time data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create single numpy array containing all the concentration data, [A] followed by [B]\n",
    "ConcA_B=np.concatenate((ConcA,ConcB))\n",
    "\n",
    "#create single numpy array containing all the time data (append 'time' to itself)\n",
    "TimeA_B=np.concatenate((time,time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Define fit function which performs the simultaneous fit of the data\n",
    "We now need to define a new function, ``simFit``, which handles the simultaneous fit of all the data. The arguments for this function are the combined independent variable array (combinedTimeData), and all the fit parameters ($k$ only in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our simultaneous fit function (simFit), and declare the independent variable, combinedTimeData, followed \n",
    "#by the single fit parameter, k\n",
    "def simFit(combinedTimeData,k):\n",
    "#split our independent varible back into its original arrays\n",
    "    time_1=combinedTimeData[:len(ConcA)] #takes the first n points from 'combinedXData' (n=length of 'ConcA')\n",
    "    time_2=combinedTimeData[len(ConcB):] #takes the last m points from 'combinedXData' (m=length of 'ConcB')\n",
    "\n",
    "#use the two extracted independent variable arrays in their respective fit functions\n",
    "    result1=expDecay(time_1,k) #'A' data\n",
    "    result2=expGrowth(time_2,k) #'B' data\n",
    "    \n",
    "    return np.append(result1,result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Fit the data\n",
    "As before, we need to supply an initial guess for the single fit parameter (we know from above that $k\\approx 0.2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessParams=[0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the ```curve_fit``` function in the usual manner, supplying the simultaneous fit function, combined x-data, combined y-data, and the fit parameter guesses as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the data to the simultaneous fit function (simFit) using the combined x-data (TimeA_B), \n",
    "#combined y-data (ConcA_B), and our guessed fit parameters (guessParams)\n",
    "popt,pcov=curve_fit(simFit,TimeA_B,ConcA_B,guessParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, 'popt' contains the global best-fit value for $k$, and 'pcov' is related to its uncertainty:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the global best-fit lines\n",
    "simFitA=np.exp(-popt[0]*time)\n",
    "simFitB=1-np.exp(-popt[0]*time)\n",
    "\n",
    "#plot the data and results of simultaneous fit\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(time,ConcA, s=60,c='#D9514E',alpha=0.5,label='[A]')\n",
    "plt.plot(time,simFitA,lw=3,c='#D9514E')\n",
    "\n",
    "plt.scatter(time,ConcB,s=60,c='#2A2B2D',alpha=0.5,label='[B]')\n",
    "plt.plot(time,simFitB,lw=3,c='#2A2B2D')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('Time / s',fontsize=18) \n",
    "plt.ylabel('Concentration / a.u.',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print the overall best-fit value for k\n",
    "err=np.sqrt(np.diag(pcov))\n",
    "print('k ={:4.3f}, 1 S.D uncertainty={:4.3f}'.format(popt[0],err[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that fitting the two data sets simultaneously reveals the best fit value for the rate coefficient to be $k=0.191\\pm0.006 \\textrm{s}^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. A more complicated example: consecutive first-order processes, A &rarr; B &rarr; C \n",
    "\n",
    "Consider the following series of irreversible chemical reactions:\n",
    "\n",
    "$$A^{*}\\xrightarrow{k_{1}}B\\xrightarrow{k_{2}}C$$\n",
    "\n",
    "These rate equations can be solved analytically. If we assume that at time $t=0$, $[A]=[A]_{0}$, and $[B]=[C]=0$, then the time dependent concentrations of $A,\\,B$ and $C$ are given by:\n",
    "\n",
    "$$[A]_t=[A]_{0}e^{-k_{1}t}$$\n",
    "\n",
    "$$[B]_t=\\frac{k_{1}[A]_{0}}{k_{2}-k_{1}}\\left\\{e^{-k_{1}t}-e^{-k_{2}t}\\right\\}$$\n",
    "\n",
    "$$[C]_t=\\left\\{1+\\frac{k_{1}e^{-k_{2}t}-k_{2}e^{-k_{1}t}}{k_{2}-k_{1}}\\right\\}[A]_{0}$$\n",
    "\n",
    "Again, to simplify things we shall assume that $[A]_0=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up simulated data sets (with noise)\n",
    "time=np.linspace(0,25,50)\n",
    "k1=np.abs(np.random.uniform(0.25,0.6)) #random number between 0.25 and 0.6\n",
    "k2=np.abs(np.random.uniform(0.1,0.3)) #random number between 0.1 and 0.3\n",
    "\n",
    "ConcA=np.exp(-k1*time)+np.random.normal(size=len(time), scale=0.03)\n",
    "ConcB=(k1/(k2-k1))*(np.exp(-k1*time)-np.exp(-k2*time))+np.random.normal(size=len(time), scale=0.03)\n",
    "ConcC=1+((k1*np.exp(-k2*time)-k2*np.exp(-k1*time))/(k2-k1))+np.random.normal(size=len(time), scale=0.03)\n",
    "\n",
    "#Plot simulated data \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(time,ConcA,s=60,c='#D9514E',label='[A]')\n",
    "plt.scatter(time,ConcB,s=60,c='#2A2B2D',label='[B]')\n",
    "plt.scatter(time,ConcC,s=60,c='#2DA8D8',label='[C]')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('Time / s',fontsize=18) \n",
    "plt.ylabel('Concentration / a.u.',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Defining the fit functions for each curve\n",
    "\n",
    "As always, we first need to define our fit functions. In our kinetic scheme there are two fit parameters in total, $k_{1}$ and $k_2$. All three fit functions must include <i>both</i> of these as arguments, even if the function itself doesn't make use of all of them. For instance, the population of $A$ depends upon $k_{1}$ <b>only</b>, but the user defined fit function still needs to include $k_2$ in its initial definition ```def popA(x,k1,k2):```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define fit functions for three populations\n",
    "#Each function must be called with all the fit parameters used in the global fit, not all parameters\n",
    "#are used by each fit function\n",
    "def popA(t,k1,k2): \n",
    "    return np.exp(-k1*t)\n",
    "\n",
    "def popB(t,k1,k2):\n",
    "    return (k1/(k2-k1))*(np.exp(-k1*t)-np.exp(-k2*t))\n",
    "\n",
    "def popC(t,k1,k2):\n",
    "    return 1+((k1*np.exp(-k2*t)-k2*np.exp(-k1*t))/(k2-k1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess at the fit paramaters and check by plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial fit guesses, [k1,k2]\n",
    "guessParams=[0.4,0.2]\n",
    "\n",
    "#Test initial guesses\n",
    "trialA=np.exp(-guessParams[0]*time)\n",
    "trialB=(guessParams[0]/(guessParams[1]-guessParams[0]))*(np.exp(-guessParams[0]*time)-np.exp(-guessParams[1]*time))\n",
    "trialC=1+((guessParams[0]*np.exp(-guessParams[1]*time)-guessParams[1]\\\n",
    "            *np.exp(-guessParams[0]*time))/(guessParams[1]-guessParams[0]))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(time,ConcA,s=60,c='#D9514E',alpha=0.5,label='[A]')\n",
    "plt.plot(time,trialA,lw=3,c='#D9514E')\n",
    "\n",
    "plt.scatter(time,ConcB,s=60,c='#2A2B2D',alpha=0.5,label='[B]')\n",
    "plt.plot(time,trialB,lw=3,c='#2A2B2D')\n",
    "\n",
    "plt.scatter(time,ConcC,s=60,c='#2DA8D8',alpha=0.5,label='[C]')\n",
    "plt.plot(time,trialC,lw=3,c='#2DA8D8')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('Time / s',fontsize=18) \n",
    "plt.ylabel('Concentration / a.u.',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Preparing the data for fitting, and curve fitting\n",
    "\n",
    "We need to combine all of the data into a single array, and pass this data to the global fit function ```simFit```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the concentrations into a single array\n",
    "ConcABC=np.concatenate((ConcA,ConcB,ConcC)) \n",
    "#repeat for the time data\n",
    "timeABC=np.concatenate((time,time,time))\n",
    "\n",
    "#define our simultaneous fitting function. The arguments are the combined time data and both of the fit parameters\n",
    "def simFit(combinedTimeData,k1,k2):\n",
    "    #split our independent variable back into its original arrays\n",
    "    time_A=combinedTimeData[0:len(ConcA)] #Extract the first data set from combinedTimeData\n",
    "    time_B=combinedTimeData[len(ConcA):2*len(ConcA)] #Extract the second dataset (all our data sets are the same length)\n",
    "    time_C=combinedTimeData[2*len(ConcA):3*len(ConcA)]\n",
    "    \n",
    "    #Use the popA, popB, and popC functions defined earlier\n",
    "    result1=popA(time_A,k1,k2)\n",
    "    result2=popB(time_B,k1,k2)\n",
    "    result3=popC(time_C,k1,k2)\n",
    "    \n",
    "    return np.append(result1,[result2,result3])\n",
    "\n",
    "#Fit the data \n",
    "popt,pcov=curve_fit(simFit,timeABC,ConcABC,guessParams)\n",
    "\n",
    "#Plot the data and best-fit curves \n",
    "simFitA=np.exp(-popt[0]*time)\n",
    "simFitB=(popt[0]/(popt[1]-popt[0]))*(np.exp(-popt[0]*time)-np.exp(-popt[1]*time))\n",
    "simFitC=1+((popt[0]*np.exp(-popt[1]*time)-popt[1]*np.exp(-popt[0]*time))/(popt[1]-popt[0]))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(time,ConcA,s=60,c='#D9514E',alpha=0.5,label='[A]')\n",
    "plt.plot(time,simFitA,lw=3,c='#D9514E')\n",
    "\n",
    "plt.scatter(time,ConcB,s=60,c='#2A2B2D',alpha=0.5,label='[B]')\n",
    "plt.plot(time,simFitB,lw=3,c='#2A2B2D')\n",
    "\n",
    "plt.scatter(time,ConcC,s=60,c='#2DA8D8',alpha=0.5,label='[C]')\n",
    "plt.plot(time,simFitC,lw=3,c='#2DA8D8')\n",
    "\n",
    "#add axis labels and a plot legend, and make the font larger\n",
    "plt.xlabel('Time / s',fontsize=18) \n",
    "plt.ylabel('Concentration / a.u.',fontsize=18)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(frameon=False,fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "#Determine the 1 S.D. errors and print along with the best fit vlaues for k1 and k2\n",
    "err=np.sqrt(np.diag(pcov))\n",
    "print('k1 ={:4.3f}, 1 S.D uncertainty={:4.3f}'.format(popt[0],err[0]))\n",
    "print('k2={:4.3f}, 1 S.D uncertainty={:4.3f}'.format(popt[1],err[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### <font>Task 5a:</font>\n",
    "### Parallel first order processes: A&rarr; B; A&rarr; C\n",
    "\n",
    "Consider the situation where our initially excited species can decay via two different pathways (<i>e.g.</i> fluorescence and intersystem crossing) simultaneously:\n",
    "\n",
    "$$A\\xrightarrow{k_{1}}B$$\n",
    "and\n",
    "$$A\\xrightarrow{k_{2}}C$$\n",
    "\n",
    "The data in the file ```parallelDecay.csv``` (located in the STUFF folder) contains some data simulating this type of kinetic chain. Read the data, and plot the concentrations of species $A$, $B$, and $C$ as a function of time. \n",
    "    \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5a answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### <font>Task 5b:</font>\n",
    "\n",
    "The solutions to this kinetic system (if we once again assume that at time $t=0$, $[A]=[A]_{0}=1$, and $[B]=[C]=0$) are:\n",
    "\n",
    "$$[A]_t=e^{-(k_{1}+k_{2})t}$$\n",
    "\n",
    "$$[B]_t=\\frac{k_{1}}{k_{1}+k_{2}}\\left(1-e^{-(k1 + k2)t}\\right)$$\n",
    "\n",
    "$$[C]_t=\\frac{k_{2}}{k_{1}+k_{2}}\\left(1-e^{-(k1 + k2)t}\\right)$$\n",
    "\n",
    "Use these functions to simultaneously fit all three data sets to extract the global best-fit values for $k_{1}$ and $k_{2}$. Plot the results of your fit, and report the best-fit values for $k_{1}$ and $k_{2}$ along with their one standard-deviation uncertaities.\n",
    "    \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5b answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
